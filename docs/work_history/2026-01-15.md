# 작업 이력 - 2026-01-15

## 10:00 - Terraform Bootstrap 모듈 생성 및 워크플로우 수정

### 배경

GitHub Actions에서 Terraform 배포 실패. S3 버킷 `jsj-lgtm-terraform-state`가 존재하지 않아 terraform init 실패.

### 변경 사항

#### 1. Bootstrap 모듈 생성

- 생성된 폴더: `ecs-migration/terraform/environments/prod/00-bootstrap/`
- 생성된 파일:
  - `main.tf` - S3 버킷 및 DynamoDB 테이블 생성
  - `variables.tf` - 변수 정의
  - `outputs.tf` - 출력값 정의

**main.tf 주요 내용:**

```hcl
terraform {
  required_version = ">= 1.0.0"
  backend "s3" {
    key = "bootstrap/terraform.tfstate"
  }
}

resource "aws_s3_bucket" "terraform_state" {
  bucket = var.state_bucket
}

resource "aws_dynamodb_table" "terraform_lock" {
  name         = var.state_lock_table
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"
}
```

#### 2. common.tfvars 버킷명 변경

- 변경된 파일: `ecs-migration/terraform/environments/prod/common.tfvars`
- 변경 내용:
  - `state_bucket = "jsj-lgtm-terraform-state-prod"`
  - `state_lock_table = "jsj-lgtm-terraform-lock-prod"`

#### 3. Bootstrap 전용 S3 버킷 수동 생성

- 버킷명: `jsj-lgtm-terraform-bootstrap`
- 목적: Bootstrap 모듈의 state 파일 저장 (chicken-and-egg 문제 해결)
- 생성 방법: AWS CLI (`aws s3 mb s3://jsj-lgtm-terraform-bootstrap`)

#### 4. terraform-component.yaml 수정

- 변경된 파일: `.github/workflows/terraform-component.yaml`
- 변경 내용: Plan, Apply, Destroy job의 terraform init에 조건부 로직 추가

**조건부 로직:**

```yaml
- name: Terraform Init
  run: |
    TFVARS="../common.tfvars"
    AWS_REGION=$(grep '^aws_region' $TFVARS | cut -d'"' -f2)

    # Bootstrap은 별도 버킷 사용 (DynamoDB locking 없음)
    if [ "${{ inputs.component }}" == "00-bootstrap" ]; then
      terraform init -upgrade \
        -backend-config="bucket=jsj-lgtm-terraform-bootstrap" \
        -backend-config="region=${AWS_REGION}" \
        -backend-config="encrypt=true"
    else
      STATE_BUCKET=$(grep '^state_bucket' $TFVARS | cut -d'"' -f2)
      STATE_LOCK_TABLE=$(grep '^state_lock_table' $TFVARS | cut -d'"' -f2)
      terraform init -upgrade \
        -backend-config="bucket=${STATE_BUCKET}" \
        -backend-config="region=${AWS_REGION}" \
        -backend-config="dynamodb_table=${STATE_LOCK_TABLE}" \
        -backend-config="encrypt=true"
    fi
```

#### 5. terraform.yaml 워크플로우 수정

- 변경된 파일: `.github/workflows/terraform.yaml`
- 변경 내용: `00-bootstrap` 컴포넌트 옵션 추가

#### 6. IAM 정책 권한 추가

- 변경 대상: `jsj_github_action_LGTM` IAM 정책
- 추가된 권한:
  - `dynamodb:CreateTable`
  - `dynamodb:DeleteTable`
  - `dynamodb:TagResource`
  - `dynamodb:UntagResource`
  - `dynamodb:UpdateTable`

### 아키텍처

```text
Bootstrap 모듈 구조:
┌──────────────────────────────────────────────────────┐
│  jsj-lgtm-terraform-bootstrap (수동 생성)            │
│  └── bootstrap/terraform.tfstate                    │
└──────────────────────────────────────────────────────┘
                        │
                        ▼ (Bootstrap 모듈이 생성)
┌──────────────────────────────────────────────────────┐
│  jsj-lgtm-terraform-state-prod                       │
│  └── lgtm-ecs/prod/XX-component/terraform.tfstate   │
└──────────────────────────────────────────────────────┘
┌──────────────────────────────────────────────────────┐
│  jsj-lgtm-terraform-lock-prod (DynamoDB)            │
└──────────────────────────────────────────────────────┘
```

### 영향도

- 영향받는 컴포넌트:
  - GitHub Actions Terraform 워크플로우
  - 전체 Terraform 컴포넌트 (state backend 변경)
- 주의사항:
  - Bootstrap은 다른 컴포넌트보다 먼저 실행해야 함
  - Bootstrap의 state는 별도 버킷(`jsj-lgtm-terraform-bootstrap`)에 저장
  - Bootstrap 버킷은 DynamoDB locking 없이 운영

---

## 15:00 - Tempo OTLP 4318 포트 제거 (Port 80 통합)

### 배경

Tempo OTLP HTTP(4318) 포트가 별도 리스너로 구성되어 있었으나, Port 80의 path-based routing (`/v1/traces`)으로 통합 가능하여 불필요한 포트 제거.

### 변경 사항

#### 1. ALB 모듈 수정

- 변경된 파일: `ecs-migration/terraform/modules/alb/main.tf`
- 삭제된 리소스:
  - `aws_lb_target_group.tempo_otlp` (4318 target group)
  - `aws_lb_listener.tempo_otlp` (4318 listener)

#### 2. ALB outputs 수정

- 변경된 파일: `ecs-migration/terraform/modules/alb/outputs.tf`
- 삭제된 출력:
  - `tempo_otlp_target_group_arn`
  - `target_group_arns` map에서 `tempo_otlp` 제거

#### 3. ECS 모듈 수정

- 변경된 파일: `ecs-migration/terraform/modules/ecs/variables.tf`
- 삭제된 변수: `tempo_otlp_target_group_arn`

- 변경된 파일: `ecs-migration/terraform/modules/ecs/main.tf`
- 삭제된 설정: Tempo 서비스의 OTLP 4318 load_balancer 블록

#### 4. common.tfvars 수정

- 변경된 파일: `ecs-migration/terraform/environments/prod/common.tfvars`
- 삭제된 항목:
  - `alb_ingress_rules.tempo_otlp` (ALB 보안 그룹)
  - `ecs_service_ports.tempo_otlp_grpc` (ECS 보안 그룹)
  - `ecs_service_ports.tempo_otlp_http` (ECS 보안 그룹)

#### 5. 60-ecs main.tf 수정

- 변경된 파일: `ecs-migration/terraform/environments/prod/60-ecs/main.tf`
- 삭제된 참조: `tempo_otlp_target_group_arn`

### 변경 전후 비교

```text
변경 전:
┌─────────────────────────────────────────────┐
│              ALB                            │
│  Port 80  → Grafana, Mimir, Loki, Tempo     │
│  Port 4318 → Tempo OTLP (별도)              │
└─────────────────────────────────────────────┘

변경 후:
┌─────────────────────────────────────────────┐
│              ALB                            │
│  Port 80  → 모든 서비스 통합                │
│    /api/v1/push    → Mimir                  │
│    /loki/*         → Loki                   │
│    /v1/traces      → Tempo (OTLP 포함)      │
│    /tempo/*        → Tempo                  │
│    /pyroscope/*    → Pyroscope              │
│    /               → Grafana (기본)         │
└─────────────────────────────────────────────┘
```

### 클라이언트 설정 변경

```text
변경 전: OTEL_EXPORTER_OTLP_ENDPOINT=http://<ALB>:4318
변경 후: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT=http://<ALB>/v1/traces
```

### 영향도

- 영향받는 컴포넌트:
  - ALB (50-alb) - 리스너 및 타겟 그룹 제거
  - ECS (60-ecs) - load_balancer 설정 제거
  - Security Group (30-security-groups) - 4318 포트 규칙 제거
- 배포 순서: ALB → ECS 순으로 재배포 필요

---

## 16:00 - Tenant ID 통일 및 Pyroscope 멀티테넌시 설정

### 배경

S3 버킷에서 `anonymous/phlaredb/` 폴더 발견. Pyroscope에 tenant ID가 설정되지 않아 기본값 `anonymous`로 저장됨. 또한 기존 tenant ID(`aws-rag-cloudwatch`, `aws-rag-ec2`)가 더 이상 사용되지 않아 통일 필요.

### 변경 사항

#### 1. common.tfvars tenant ID 변경

- 변경된 파일: `ecs-migration/terraform/environments/prod/common.tfvars`
- 변경 내용:

```hcl
# 변경 전
alloy_config = {
  loki_tenant  = "aws-rag-cloudwatch"
  mimir_tenant = "aws-rag-ec2"
}

# 변경 후
alloy_config = {
  loki_tenant  = "jsj-lgtm-header"
  mimir_tenant = "jsj-lgtm-header"
}
```

#### 2. Grafana Pyroscope 데이터소스에 tenant 헤더 추가

- 변경된 파일: `ecs-migration/dockerfiles/grafana/config/provisioning/datasources/datasources.yaml`
- 변경 내용: Pyroscope 데이터소스에 `X-Scope-OrgID` 헤더 추가

```yaml
# 변경 전
- name: Pyroscope
  type: grafana-pyroscope-datasource
  access: proxy
  url: ${PYROSCOPE_DATASOURCE_URL}
  editable: false

# 변경 후
- name: Pyroscope
  type: grafana-pyroscope-datasource
  access: proxy
  url: ${PYROSCOPE_DATASOURCE_URL}
  jsonData:
    httpHeaderName1: X-Scope-OrgID
  secureJsonData:
    httpHeaderValue1: ${LGTM_TENANT_ID}
  editable: false
```

### Tenant ID 구조 (변경 후)

| 서비스 | Tenant ID | 용도 |
|--------|-----------|------|
| Mimir | jsj-lgtm-header | 메트릭 저장 |
| Loki | jsj-lgtm-header | 로그 저장 |
| Tempo | jsj-lgtm-header | 트레이스 저장 |
| Pyroscope | jsj-lgtm-header | 프로파일 저장 |

### 영향도

- 영향받는 컴포넌트:
  - Grafana (datasources 설정)
  - Alloy (tenant ID 변경)
  - ECS 서비스 재배포 필요 (환경변수 변경)
- 주의사항:
  - 기존 `aws-rag-*` tenant의 데이터는 새 tenant에서 조회 불가
  - 새 데이터는 `jsj-lgtm-header/` 폴더에 저장됨

---

## 18:00 - ElastiCache Memcached 구성 및 LGTM 스택 설정 개선

### 배경

LGTM 스택(Mimir, Loki, Tempo, Pyroscope) 설정을 Grafana Labs 권고사항에 맞게 검토. Memcached 캐싱이 설정 파일에는 참조되어 있으나 실제 ElastiCache가 구성되지 않아 쿼리 성능 저하. 또한 Tempo S3 endpoint 오타, 하드코딩된 환경변수 등 개선 필요.

### 변경 사항

#### 1. ElastiCache Terraform 모듈 생성

- 생성된 폴더: `ecs-migration/terraform/modules/elasticache/`
- 생성된 파일:
  - `main.tf` - ElastiCache Memcached 클러스터, 서브넷 그룹, 보안 그룹, 파라미터 그룹
  - `variables.tf` - 변수 정의 (node_type, memcached_version 등)
  - `outputs.tf` - 출력값 (primary_endpoint, cluster_id 등)

```hcl
resource "aws_elasticache_cluster" "memcached" {
  cluster_id           = "${var.project_name}-${var.environment}-memcached"
  engine               = "memcached"
  engine_version       = var.memcached_version  # 1.6.22
  node_type            = var.node_type          # cache.t3.micro
  num_cache_nodes      = var.num_cache_nodes    # 1
  port                 = 11211
  parameter_group_name = aws_elasticache_parameter_group.main.name
  subnet_group_name    = aws_elasticache_subnet_group.main.name
  security_group_ids   = [aws_security_group.memcached.id]
}
```

#### 2. ElastiCache 루트 모듈 생성 (35-elasticache)

- 생성된 폴더: `ecs-migration/terraform/environments/prod/35-elasticache/`
- 생성된 파일: `main.tf`, `variables.tf`, `outputs.tf`
- 실행 순서: 5번째 (30-security-groups 이후, 40-cloudmap 이전)
- 의존성: 01-vpc, 30-security-groups

#### 3. common.tfvars에 Memcached 설정 추가

```hcl
memcached_config = {
  version            = "1.6.22"
  node_type          = "cache.t3.micro"
  num_cache_nodes    = 1
  max_item_size      = 1048576  # 1MB
  maintenance_window = "sun:05:00-sun:06:00"
}
```

#### 4. ECS 모듈 수정

- 변경된 파일: `ecs-migration/terraform/modules/ecs/variables.tf`
  - `memcached_endpoint` 변수 추가

- 변경된 파일: `ecs-migration/terraform/modules/ecs/main.tf`
  - Mimir Task Definition에 `MEMCACHED_ENDPOINT` 환경변수 추가
  - Loki Task Definition에 `MEMCACHED_ENDPOINT` 환경변수 추가

#### 5. 60-ecs 루트 모듈 수정

- 변경된 파일: `ecs-migration/terraform/environments/prod/60-ecs/main.tf`
  - ElastiCache remote state 데이터 소스 추가
  - `memcached_endpoint` 모듈 입력 추가

#### 6. Mimir 설정 개선 (캐싱 추가)

- 변경된 파일: `lgtm-stack/mimir/config/mimir-config.yaml`
- 추가된 설정:

```yaml
blocks_storage:
  bucket_store:
    index_cache:
      backend: memcached
      memcached:
        addresses: ${MEMCACHED_ENDPOINT}
        max_idle_connections: 100
    chunks_cache:
      backend: memcached
      memcached:
        addresses: ${MEMCACHED_ENDPOINT}
        max_idle_connections: 100
    metadata_cache:
      backend: memcached
      memcached:
        addresses: ${MEMCACHED_ENDPOINT}
```

#### 7. Loki 설정 개선

- 변경된 파일: `lgtm-stack/loki/config/loki-config.yaml`
- 변경 내용:
  - `path_prefix: /ftt-loki` → `/loki`
  - `chunk_idle_period: 2h` → `30m` (Grafana Labs 기본값)
  - `storage_prefix: ftt-loki` → `loki`
  - Memcached 주소: `ftt-mimir-memcache:11211` → `${MEMCACHED_ENDPOINT}`
  - querier 설정 추가: `max_concurrent: 16`, `query_timeout: 2m`

#### 8. Tempo 설정 개선 (Critical Fix)

- 변경된 파일: `lgtm-stack/tempo/config/tempo-config.yaml`
- **Critical Fix**: S3 endpoint 오타 수정
  - `s3.northeast-2.amazonaws.com` → `s3.${AWS_REGION}.amazonaws.com`
- `auth_enabled: false` → `true` (multitenancy_enabled와 일관성)
- 하드코딩 제거:
  - `bucket: sys-lgtm-s3` → `${TEMPO_S3_BUCKET}`
  - `remote_write url: http://10.130.30.62:9009/...` → `${MIMIR_REMOTE_WRITE_URL}`
- 메모리 보호 설정 추가: `max_recv_msg_size_mib: 16`

#### 9. Pyroscope 설정 개선

- 변경된 파일: `lgtm-stack/pyroscope/config/pyroscope-config.yaml`
- 하드코딩 제거:
  - `bucket_name: sys-lgtm-s3` → `${PYROSCOPE_S3_BUCKET}`
  - `region: ap-northeast-2` → `${AWS_REGION}`
- prefix 변경: `ftt-pyroscope` → `pyroscope`

#### 10. GitHub Actions 워크플로우 업데이트

- 변경된 파일: `.github/workflows/terraform.yaml`
- 변경 내용:
  - workflow_dispatch에 `35-elasticache` 옵션 추가
  - `stage3-elasticache` job 추가 (Apply)
  - `destroy-stage3-elasticache` job 추가 (Destroy)
  - stage5-ecs에 elasticache 의존성 추가
  - Summary에 ElastiCache 결과 추가

### 아키텍처 변경

```text
변경 전:
┌─────────────────────────────────────────────┐
│  Mimir/Loki Config                          │
│  memcached: ftt-mimir-memcache:11211 (없음) │
└─────────────────────────────────────────────┘

변경 후:
┌─────────────────────────────────────────────┐
│  ElastiCache Memcached                      │
│  - Cluster: jsj-lgtm-prod-memcached         │
│  - Node Type: cache.t3.micro                │
│  - Engine: 1.6.22                           │
└─────────────────────────────────────────────┘
         │
         ▼ (환경변수: MEMCACHED_ENDPOINT)
┌─────────────────────────────────────────────┐
│  Mimir: index/chunks/metadata 캐싱           │
│  Loki: index/chunks 캐싱                     │
└─────────────────────────────────────────────┘
```

### 배포 순서

1. 35-elasticache apply (ElastiCache 클러스터 생성)
2. Docker 이미지 리빌드 (설정 파일 변경 반영)
3. 60-ecs apply (ECS 서비스 재배포)

### 영향도

- 영향받는 컴포넌트:
  - ElastiCache (신규)
  - Mimir (캐싱 활성화)
  - Loki (캐싱 활성화)
  - Tempo (S3 endpoint 수정, 환경변수화)
  - Pyroscope (환경변수화)
  - GitHub Actions (워크플로우 추가)
- 예상 효과:
  - 쿼리 성능 향상 (캐싱)
  - Tempo S3 연결 오류 해결
  - 환경별 설정 유연성 향상

---

## 19:00 - ECS Fargate 베스트 프랙티스 적용

### 배경

Grafana Labs 공식 문서 기반으로 ECS Fargate 환경에서의 LGTM 스택 베스트 프랙티스 검토 후 개선사항 적용.

### 검토 결과

#### 이미 적용된 항목 (양호)

- ECS stopTimeout: 120초 (Fargate 최대값)
- Loki WAL: 활성화됨 (`flush_on_shutdown: true`)
- Mimir Compactor: 설정됨 (`deletion_delay: 12h`, `cleanup_interval: 15m`)
- Tempo metrics-generator: span-metrics, service-graphs 활성화
- Memcached 캐싱: 모든 캐시 레이어 설정됨

#### 이번에 적용한 항목

1. **ALB Deregistration Delay 설정**
2. **ECS 리소스 50% 메모리 버퍼**

### 변경 사항

#### 1. ALB Target Group Deregistration Delay 추가

- 변경된 파일: `ecs-migration/terraform/modules/alb/main.tf`
- 변경 내용: 모든 target group에 `deregistration_delay` 추가
- 적용 대상: grafana, mimir, loki, tempo, pyroscope

```hcl
resource "aws_lb_target_group" "grafana" {
  # ... 기존 설정
  deregistration_delay = var.deregistration_delay  # 120초 (ECS stopTimeout과 동일)
  # ...
}
```

- 변경된 파일: `ecs-migration/terraform/modules/alb/variables.tf`
- 추가된 변수:

```hcl
variable "deregistration_delay" {
  description = "Deregistration delay in seconds (should match ECS stopTimeout)"
  type        = number
  default     = 120
}
```

**효과:**

- ALB가 타겟 해제 시 120초 동안 기존 연결 drain
- ECS stopTimeout(120s)과 동일하게 설정하여 graceful shutdown 보장
- 배포 중 데이터 손실 방지

#### 2. ECS 서비스 메모리 50% 버퍼 추가

- 변경된 파일: `ecs-migration/terraform/environments/prod/common.tfvars`
- 변경 내용: Grafana Labs 권장사항에 따라 메모리 50% 여유분 추가

| 서비스 | 변경 전 Memory | 변경 후 Memory | 증가분 |
|--------|----------------|----------------|--------|
| mimir | 2048 MB | 3072 MB | +50% |
| loki | 1024 MB | 1536 MB | +50% |
| tempo | 1024 MB | 1536 MB | +50% |
| pyroscope | 1024 MB | 1536 MB | +50% |
| grafana | 1024 MB | 1536 MB | +50% |
| alloy | 512 MB | 768 MB | +50% |

```hcl
service_config = {
  mimir = {
    cpu            = 1024
    memory         = 3072    # 2048 + 50% 버퍼
    desired_count  = 1
    container_port = 8080
  }
  # ... 다른 서비스들도 동일하게 적용
}
```

**효과:**

- GC 압박 감소
- OOM 킬 방지
- 갑작스러운 트래픽 스파이크 대응 가능

### 미적용 항목 (선택적)

- Loki/Tempo Flush Endpoint 활용: 현재 WAL flush_on_shutdown으로 충분
- SIGTERM 핸들러: 각 서비스 자체 구현되어 있음

### 영향도

- 영향받는 컴포넌트:
  - ALB (50-alb) - deregistration_delay 설정 추가
  - ECS (60-ecs) - 메모리 리소스 증가
- 비용 영향:
  - Fargate 메모리 비용 약 50% 증가
  - 안정성 향상으로 장애 대응 비용 절감 기대
- 배포 순서:
  1. ALB apply (target group 업데이트)
  2. ECS apply (서비스 리소스 업데이트)

---

## 19:30 - Mimir/Loki S3 Endpoint 하드코딩 제거

### 배경

설정 파일에 `ap-northeast-2` 리전이 하드코딩되어 있어 환경변수로 대체 필요.

### 변경 사항

#### 1. Mimir 설정 수정

- 변경된 파일: `lgtm-stack/mimir/config/mimir-config.yaml`
- 변경 내용:

```yaml
# 변경 전
common:
  storage:
    s3:
      endpoint: s3.ap-northeast-2.amazonaws.com
      region: ap-northeast-2

# 변경 후
common:
  storage:
    s3:
      endpoint: s3.${AWS_REGION}.amazonaws.com
      region: ${AWS_REGION}
```

#### 2. Loki 설정 수정

- 변경된 파일: `lgtm-stack/loki/config/loki-config.yaml`
- 변경 내용:

```yaml
# 변경 전
storage_config:
  object_store:
    s3:
      endpoint: s3.ap-northeast-2.amazonaws.com

# 변경 후
storage_config:
  object_store:
    s3:
      endpoint: s3.${AWS_REGION}.amazonaws.com
```

### 영향도

- 영향받는 컴포넌트: Mimir, Loki (Docker 이미지 리빌드 필요)
- ECS Task Definition에서 `AWS_REGION` 환경변수 주입됨

---

## 20:00 - LGTM 스택 설정 개선 및 주석 스타일 통일

### 변경 사항

#### 1. Tempo WAL 설정 추가

- 변경된 파일: `lgtm-stack/tempo/config/tempo-config.yaml`
- 추가된 설정:

```yaml
ingester:
  lifecycler:
    ring:
      replication_factor: 1               # 단일 노드 모드
```

#### 2. Pyroscope 멀티테넌시 활성화

- 변경된 파일: `lgtm-stack/pyroscope/config/pyroscope-config.yaml`
- 추가된 설정:

```yaml
multitenancy_enabled: true                # 멀티테넌트 활성화
auth_enabled: true                        # 인증 활성화
```

#### 3. 설정 파일 주석 스타일 통일

모든 LGTM 스택 설정 파일의 주석 스타일을 통일:

- 헤더: `###############################################################################`
- 섹션 번호: `# 1. 섹션명`, `# 2. 섹션명` 형식
- 인라인 주석: 한글로 통일
- 적용 파일:
  - `lgtm-stack/mimir/config/mimir-config.yaml`
  - `lgtm-stack/loki/config/loki-config.yaml`
  - `lgtm-stack/tempo/config/tempo-config.yaml`
  - `lgtm-stack/pyroscope/config/pyroscope-config.yaml`

### 주석 스타일 예시

```yaml
###############################################################################
# Mimir Configuration - ECS Fargate 호환
###############################################################################

###############################################################################
# 1. 공통 S3 백엔드 설정
###############################################################################
common:
  storage:
    backend: s3
    s3:
      endpoint: s3.${AWS_REGION}.amazonaws.com
      region: ${AWS_REGION}
      bucket_name: ${MIMIR_S3_BUCKET}
```

### 영향도

- 영향받는 컴포넌트: 모든 LGTM 스택 (Docker 이미지 리빌드 필요)
- Pyroscope: 이제 `X-Scope-OrgID` 헤더 필수

---

## 24:00 - ECS Fargate CPU/Memory 유효 조합 수정

### 문제 상황

ECS 배포 실패. Terraform Apply 시 Fargate Task Definition 생성 오류 발생.

**에러 메시지:**

```text
Error: creating ECS Task Definition (lgtm-loki): ClientException: No Fargate configuration exists for given values: 512 CPU, 1536 memory.
Error: creating ECS Task Definition (lgtm-alloy): ClientException: No Fargate configuration exists for given values: 256 CPU, 768 memory.
```

### 원인

AWS Fargate는 특정 CPU/Memory 조합만 허용함. 이전에 적용한 "50% 메모리 버퍼" 설정이 Fargate 유효 조합을 벗어남.

**Fargate 유효 조합:**

| CPU | 허용 Memory |
|-----|-------------|
| 256 | 512, 1024, 2048 |
| 512 | 1024, 2048, 3072, 4096 |
| 1024 | 2048, 3072, 4096, 5120, 6144, 7168, 8192 |

### 수정 내용

- 변경된 파일: `ecs-migration/terraform/environments/prod/common.tfvars`
- 변경 내용: Fargate 유효 조합에 맞게 메모리 조정

| 서비스 | 변경 전 | 변경 후 | 비고 |
|--------|---------|---------|------|
| loki | 512 CPU / 1536 MB | 512 CPU / 2048 MB | 유효 조합 |
| tempo | 512 CPU / 1536 MB | 512 CPU / 2048 MB | 유효 조합 |
| pyroscope | 512 CPU / 1536 MB | 512 CPU / 2048 MB | 유효 조합 |
| grafana | 512 CPU / 1536 MB | 512 CPU / 2048 MB | 유효 조합 |
| alloy | 256 CPU / 768 MB | 256 CPU / 1024 MB | 유효 조합 |
| mimir | 1024 CPU / 3072 MB | 변경 없음 | 이미 유효 |

```hcl
# 수정된 설정
service_config = {
  mimir = {
    cpu            = 1024
    memory         = 3072    # Fargate 유효: 1024 CPU → 2048~8192 MB
    desired_count  = 1
    container_port = 8080
  }
  loki = {
    cpu            = 512
    memory         = 2048    # Fargate 유효: 512 CPU → 1024~4096 MB
    desired_count  = 1
    container_port = 3100
  }
  # ... 나머지 서비스도 동일
}
```

### 배포 결과

GitHub Actions 워크플로우 실행 성공 (Run ID: 21023301391)

- ✅ Stage1: VPC, IAM - Apply 성공
- ✅ Stage2: CloudWatch Logs, ECR, S3 - Apply 성공
- ✅ Stage3: Security Groups, CloudMap, ElastiCache - Apply 성공
- ✅ Stage4: ALB - Apply 성공
- ✅ Stage5: ECS - Apply 성공

### 영향도

- 영향받는 컴포넌트: ECS (60-ecs)
- 비용 영향: 메모리가 1536 → 2048 MB로 증가하여 약간의 비용 증가
- 안정성: Fargate 유효 조합 사용으로 안정적 배포 보장
